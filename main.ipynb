{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os, sys, findspark, numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Java 17 & Python 3.11\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/Cellar/openjdk@17/17.0.17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "# Builds PySpark session for 4 local cores with 10GB RAM\n",
    "findspark.init()\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SpotifyRec\")\n",
    "    .master(\"local[4]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# Remove error logs for cleaner output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactions DataFrame\n",
    "playlist_tracks = spark.read.parquet(\"parquet_data/playlist_tracks\")\n",
    "playlist_tracks.createOrReplaceTempView(\"playlist_tracks\")\n",
    "\n",
    "# User DataFrame\n",
    "playlists = spark.read.parquet(\"parquet_data/playlists\")\n",
    "playlists.createOrReplaceTempView(\"playlists\")\n",
    "\n",
    "# Items DataFrame\n",
    "tracks = spark.read.parquet(\"parquet_data/tracks\")\n",
    "tracks.createOrReplaceTempView(\"tracks\")\n",
    "\n",
    "# Item Features - Track\n",
    "# Read tracks_features parquet for model training, create a temporary SQL table\n",
    "track_features = spark.read.parquet('parquet_data/track_features')\n",
    "track_features.createOrReplaceTempView(\"track_features\")\n",
    "\n",
    "# Item Features - Artist\n",
    "# Read artist_features parquet for model training, create a temporary SQL table\n",
    "artist_features = spark.read.parquet('parquet_data/artist_features')\n",
    "artist_features.createOrReplaceTempView('artist_features')\n",
    "\n",
    "# User Features\n",
    "# Read playlist_features parquet for model training, create a temporary SQL table\n",
    "playlist_features = spark.read.parquet('parquet_data/playlist_features')\n",
    "playlist_features.createOrReplaceTempView(\"playlist_features\")\n",
    "\n",
    "# Item-User Interactions\n",
    "# Read edges parquet for model training, create a temporary SQL table\n",
    "edges = spark.read.parquet('parquet_data/edges')\n",
    "edges.createOrReplaceTempView(\"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d020c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_shift = edges.withColumn(\"tid\", F.col(\"tid\") + 1)\n",
    "edges_shift.createOrReplaceTempView('edges_shift')\n",
    "track_features_shift = track_features.withColumn(\"tid\", F.col(\"tid\") + 1)\n",
    "track_features_shift.createOrReplaceTempView('track_features_shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ca491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each playlist, create an array of all track ids in that playlist\n",
    "pl_seqs = (edges_shift\n",
    "         .groupby('pid')\n",
    "         .agg(F.array_distinct(F.collect_list('tid')).alias('tids'))\n",
    "         .filter(F.size('tids') >= 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ccd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the length = 20, arrays for the tids, and create a masking object\n",
    "PAD_ID = 0\n",
    "max_length = 20\n",
    "\n",
    "\n",
    "pairs = (\n",
    "    pl_seqs\n",
    "    # 1) Pick a positive by shuffling the tids per playlist, then take the first\n",
    "    .withColumn('shuffle', F.shuffle(F.col('tids')))\n",
    "    .withColumn('pos_tid', F.element_at(F.col('shuffle'), 1))\n",
    "    # 2) Remove the positive from the context pool\n",
    "    .withColumn('remain', F.filter('tids', lambda x: x != F.col('pos_tid')))\n",
    "    # 3) Take up the max_length random items, shuffle then slice\n",
    "    .withColumn('items', F.slice(F.shuffle(F.col('remain')), 1, max_length))\n",
    "    # 4) Build mask & padding to max_length\n",
    "    .withColumn('len', F.size('items'))\n",
    "    .withColumn('pad_len', F.greatest(F.lit(0), F.lit(max_length) - F.col('len')))\n",
    "    .withColumn('tokens', F.concat(F.col('items'), F.array_repeat(F.lit(PAD_ID), F.col('pad_len'))))\n",
    "    .withColumn('mask', F.concat(F.array_repeat(F.lit(1), F.col('len')), F.array_repeat(F.lit(0), F.col('pad_len'))))\n",
    "    # 5) Select relevant items\n",
    "    .select('pid', 'tokens', 'mask', 'pos_tid')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random 95/5 train/validation split\n",
    "bucketed = pairs.withColumn('bucket', F.pmod(F.abs(F.hash('pid')), F.lit(100))) # Creates a random positive integer & modulus divides by 100. Essentially randomly groups each pid into 100 buckets\n",
    "train_pairs = bucketed.filter('bucket < 95').select('pid', 'tokens', 'mask', 'pos_tid') # ~95% of the data\n",
    "val_pairs = bucketed.filter('bucket >= 95').select('pid', 'tokens', 'mask', 'pos_tid') # ~5% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate train/val split\n",
    "print(\"train rows:\", train_pairs.count())\n",
    "print(\"val rows:\",   val_pairs.count())\n",
    "n_tracks = edges_shift.agg(F.max('tid').alias('max_tid')).collect()[0]['max_tid']\n",
    "print('n_tracks (embedding size):', int(n_tracks) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a40bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "\n",
    "PARQUET_TRAIN = 'parquet_data/train_pairs'\n",
    "PARQUET_VAL = 'parquet_data/val_pairs'\n",
    "\n",
    "n_tracks = 2262108 \n",
    "max_length = 20\n",
    "embed_dim = 128\n",
    "batch_size = 512 # raise if have GPU RAM\n",
    "# Looks for the best GPUs to train on\n",
    "DEVICE = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "class PlaylistPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parquet_path):\n",
    "        table = ds.dataset(parquet_path, format='parquet').to_table(columns=['tokens', 'mask', 'pos_tid'])\n",
    "        self.tokens = np.stack(table['tokens'].to_pylist()).astype(np.int64)\n",
    "        self.mask = np.stack(table['mask'].to_pylist()).astype(np.bool_)\n",
    "        self.pos = np.asarray(table['pos_tid']).astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.pos.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'tokens': torch.from_numpy(self.tokens[i]),\n",
    "            'mask': torch.from_numpy(self.mask[i]),\n",
    "            'pos': torch.tensor(self.pos[i], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def make_loader(path, batch_size=batch_size, shuffle=True, num_workers=0):\n",
    "    ds_ = PlaylistPairDataset(path)\n",
    "    return ds_, torch.utils.data.DataLoader(\n",
    "        ds_, \n",
    "        batch_size = batch_size, \n",
    "        shuffle=shuffle, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=(DEVICE=='cuda'), \n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "train_ds, train_loader = make_loader(PARQUET_TRAIN, shuffle=True, num_workers=0)\n",
    "val_ds, val_loader = make_loader(PARQUET_VAL, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a74ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate tensors\n",
    "b = next(iter(train_loader))\n",
    "print(b['tokens'].shape, b['mask'].shape, b['pos'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ddf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, n_tracks, embed_dim=128, mlp_hidden=256, share_embed=True):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(n_tracks, embed_dim, padding_idx=0)\n",
    "        self.user_emb = self.item_emb if share_embed else nn.Embedding(n_tracks, embed_dim, padding_idx=0)\n",
    "        self.item_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, embed_dim)\n",
    "        )\n",
    "        self.user_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, embed_dim)\n",
    "        )\n",
    "        self.tau = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def playlist_forward(self, tokens, mask):\n",
    "        e = self.user_emb(tokens)\n",
    "        e = e * mask.unsqueeze(-1)\n",
    "        denom = mask.sum(dim=1, keepdims=True).clamp_min(1.0)\n",
    "        pooled = e.sum(dim=1) / denom\n",
    "        return F.normalize(self.user_mlp(pooled), dim=-1)\n",
    "    def track_forward(self, pos_ids):\n",
    "        t = self.item_emb(pos_ids)\n",
    "        return F.normalize(self.item_mlp(t), dim=-1)\n",
    "    def forward(self, tokens, mask, pos):\n",
    "        p = self.playlist_forward(tokens, mask)\n",
    "        t = self.track_forward(pos)\n",
    "        logits = (p @ t.T) / self.tau.clamp_min(.001)\n",
    "        return logits\n",
    "    \n",
    "model = TwoTower(n_tracks, embed_dim, mlp_hidden=256, share_embed=True).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=.0003, weight_decay=.0001)\n",
    "scaler = torch.amp.GradScaler() if DEVICE=='cuda' else None\n",
    "    \n",
    "def train_one_epoch():\n",
    "    model.train(); running=0.0\n",
    "    for batch in train_loader:\n",
    "        tokens = batch['tokens'].to(DEVICE)\n",
    "        mask = batch['mask'].to(DEVICE)\n",
    "        pos = batch['pos'].to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        if scaler:\n",
    "            with torch.amp.autocast():\n",
    "                logits =  model(tokens, mask, pos)\n",
    "                targets = torch.arange(logits.size(0), device=logits.device)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            logits = model(tokens, mask, pos)\n",
    "            targets = torch.arange(logits.size(0), device=logits.device)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        running += loss.item()\n",
    "    return running / max(1, len(train_loader))\n",
    "\n",
    "def recall_at_k(loader, K=10, pool=2000):\n",
    "    model.eval()\n",
    "    cand = []\n",
    "    for i, b in enumerate(loader):\n",
    "        cand.extend(b['pos'].tolist())\n",
    "        if len(cand) >= pool:\n",
    "            break\n",
    "    cand = torch.tensor(cand[:pool], device=DEVICE)\n",
    "    \n",
    "    hits = n = 0\n",
    "    for b in loader:\n",
    "        tokens = b['tokens'].to(DEVICE) \n",
    "        mask = b['mask'].to(DEVICE)\n",
    "        pos = b['pos'].to(DEVICE)\n",
    "        p = model.playlist_forward(tokens, mask)\n",
    "        batch = pos.size(0)\n",
    "        pool_ids = cand.unsqueeze(0).repeat(batch, 1)\n",
    "        pos = pool_ids[:, 0]\n",
    "        t = model.track_forward(pool_ids.reshape(-1)).reshape(batch, -1, p.size(1))\n",
    "        scores = (p.unsqueeze(1) * t).sum(-1)\n",
    "        topk = scores.topk(K, dim=1).indices\n",
    "        hits += (topk[:, 0] == 0).sum().item()\n",
    "        n += batch\n",
    "    return hits / max(1, n)\n",
    "\n",
    "epochs = 3\n",
    "best = 0.0\n",
    "for ep in range(1, epochs+1):\n",
    "    tr = train_one_epoch()\n",
    "    r10 = recall_at_k(val_loader, K=10, pool=2000)\n",
    "    print(f'epoch {ep}: loss={tr:.4f}, val@10={r10:.4f}')\n",
    "    if r10 > best:\n",
    "        best = r10\n",
    "        torch.save({'state_dict': model.state_dict(),\n",
    "                    'n_tracks': n_tracks,\n",
    "                    'embed_dim': embed_dim},\n",
    "                   'two_tower_best.pt')\n",
    "print('best val@10', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for counts of pl_seq\n",
    "print('playlists (>= 2 tracks):', pl_seqs.count())\n",
    "size_hist = (\n",
    "    pl_seqs\n",
    "    .withColumn('n_tracks', F.size('tids'))\n",
    "    .groupBy('n_tracks').count()\n",
    "    .orderBy('n_tracks')\n",
    ")\n",
    "mins_maxs = edges_shift.agg(F.min('tid').alias('min_tid'), F.max('tid').alias('max_tid')).collect()[0]\n",
    "print(\"tid range:\", mins_maxs[\"min_tid\"], \"to\", mins_maxs[\"max_tid\"])\n",
    "size_hist.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT *\n",
    "          FROM playlist_tracks\n",
    "          ORDER BY pid\n",
    "          LIMIT 5\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44399bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT *\n",
    "          FROM edges_shift\n",
    "          ORDER BY tid ASC\n",
    "          LIMIT 5\n",
    "          \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b37000",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT *\n",
    "          FROM track_features_shift\n",
    "          ORDER BY tid ASC\n",
    "          LIMIT 5\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877254e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cac716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(edges.select('pid').distinct().count())\n",
    "print(edges.select('tid').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1906676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is essentially InfoNCE with one positive and multiple negatives per query\n",
    "def info_nce_loss(query, positive, negatives, temperature=0.07):\n",
    "    # query: [batch, dim]\n",
    "    # positive: [batch, dim]\n",
    "    # negatives: [batch, num_neg, dim]\n",
    "    \n",
    "    # Compute similarities\n",
    "    pos_sim = torch.sum(query * positive, dim=-1, keepdim=True) / temperature\n",
    "    neg_sim = torch.bmm(negatives, query.unsqueeze(-1)).squeeze(-1) / temperature  # [batch, num_neg]\n",
    "    \n",
    "    logits = torch.cat([pos_sim, neg_sim], dim=1)\n",
    "    labels = torch.zeros(query.size(0), dtype=torch.long, device=query.device)\n",
    "    \n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f9d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
