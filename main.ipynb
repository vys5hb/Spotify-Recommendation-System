{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc49826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os, sys, findspark, numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4036d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/10 00:36:43 WARN Utils: Your hostname, Ethans-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 100.64.14.129 instead (on interface en0)\n",
      "25/11/10 00:36:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 00:36:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Uses Java 17 & Python 3.11\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/Cellar/openjdk@17/17.0.17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "# Builds PySpark session for 4 local cores with 10GB RAM\n",
    "findspark.init()\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"SpotifyRec\")\n",
    "    .master(\"local[4]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# Remove error logs for cleaner output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb60d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactions DataFrame\n",
    "playlist_tracks = spark.read.parquet(\"parquet_data/playlist_tracks\")\n",
    "playlist_tracks.createOrReplaceTempView(\"playlist_tracks\")\n",
    "\n",
    "# User DataFrame\n",
    "playlists = spark.read.parquet(\"parquet_data/playlists\")\n",
    "playlists.createOrReplaceTempView(\"playlists\")\n",
    "\n",
    "# Items DataFrame\n",
    "tracks = spark.read.parquet(\"parquet_data/tracks\")\n",
    "tracks.createOrReplaceTempView(\"tracks\")\n",
    "\n",
    "# Item Features\n",
    "# Read tracks_features parquet for model training, create a temporary SQL table\n",
    "track_features = spark.read.parquet('parquet_data/track_features')\n",
    "track_features.createOrReplaceTempView(\"track_features\")\n",
    "\n",
    "# User Features\n",
    "# Read playlist_features parquet for model training, create a temporary SQL table\n",
    "playlist_features = spark.read.parquet('parquet_data/playlist_features')\n",
    "playlist_features.createOrReplaceTempView(\"playlist_features\")\n",
    "\n",
    "# Item-User Interactions\n",
    "# Read edges parquet for model training, create a temporary SQL table\n",
    "edges = spark.read.parquet('parquet_data/edges')\n",
    "edges.createOrReplaceTempView(\"edges\")\n",
    "\n",
    "# Training Interactions\n",
    "train_pairs = spark.read.parquet('parquet_data/train_pairs')\n",
    "# Validation Interactions\n",
    "val_pairs = spark.read.parquet('parquet_data/val_pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82e17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------+\n",
      "|pid|              tokens|                mask|pos_tid|\n",
      "+---+--------------------+--------------------+-------+\n",
      "|  2|[2055362, 902534,...|[1, 1, 1, 1, 1, 1...| 285459|\n",
      "|  8|[953644, 1226541,...|[1, 1, 1, 1, 1, 1...| 909553|\n",
      "| 11|[1279865, 1780757...|[1, 1, 1, 1, 1, 1...| 721318|\n",
      "| 35|[2102277, 1427277...|[1, 1, 1, 1, 1, 1...| 372978|\n",
      "| 49|[73755, 1607709, ...|[1, 1, 1, 1, 1, 1...| 152751|\n",
      "+---+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "None\n",
      "+---+--------------------+--------------------+-------+\n",
      "|pid|              tokens|                mask|pos_tid|\n",
      "+---+--------------------+--------------------+-------+\n",
      "|192|[2137547, 747563,...|[1, 1, 1, 1, 1, 1...| 146944|\n",
      "|249|[1012256, 201022,...|[1, 1, 1, 1, 1, 1...|1715753|\n",
      "|337|[1385456, 338913,...|[1, 1, 1, 1, 1, 1...| 712726|\n",
      "|352|[598275, 1742374,...|[1, 1, 1, 1, 1, 1...|1311226|\n",
      "|381|[2126317, 877997,...|[1, 1, 1, 1, 1, 1...|1856934|\n",
      "+---+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "None\n",
      "+------+---+\n",
      "|   pid|tid|\n",
      "+------+---+\n",
      "|818067| 37|\n",
      "|176550| 48|\n",
      "|450823| 53|\n",
      "|222572| 53|\n",
      "|177306|120|\n",
      "+------+---+\n",
      "only showing top 5 rows\n",
      "None\n",
      "+-------+---+------+-------------------+-------------------+\n",
      "|    tid|aid|  alid|    z_log_track_cnt|   z_log_artist_cnt|\n",
      "+-------+---+------+-------------------+-------------------+\n",
      "|1934863|  1|263455|0.16121013343064558|-0.6048881849534854|\n",
      "|1625366|  1|263455| 1.1966455465563837|-0.6048881849534854|\n",
      "|2168396|  1|263455|-0.6140627213641416|-0.6048881849534854|\n",
      "| 984631|  8|144890|-0.6140627213641416|-1.4690805334462491|\n",
      "|2133802|  9|191031|-0.6140627213641416|-1.4690805334462491|\n",
      "+-------+---+------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "None\n",
      "+------+---------+-----------------+------------------+-----------------+------------------+------------------+-------------+\n",
      "|   pid|     name|     log_n_tracks|     log_n_artists|     log_n_albums|   log_pl_duration|      log_days_mod|collaborative|\n",
      "+------+---------+-----------------+------------------+-----------------+------------------+------------------+-------------+\n",
      "|298014|     frat|4.219507705176107|3.9318256327243257|4.127134385045092|16.543618778583895| 6.280395838960195|        false|\n",
      "|298015|Christian|4.465908118654584| 3.258096538021482|4.204692619390966|16.959773778758095|3.1780538303479458|        false|\n",
      "|298016|    Party|5.187385805840755| 4.770684624465665|5.030437921392435|17.592973596252165|5.0689042022202315|        false|\n",
      "|298017|   Chilll|5.043425116919247| 4.532599493153256|4.718498871295094| 17.43764230582052|4.3694478524670215|        false|\n",
      "|298018|      RAP|2.995732273553991|2.9444389791664403|2.995732273553991| 15.29393139992934|5.4638318050256105|        false|\n",
      "+------+---------+-----------------+------------------+-----------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs.show(5))\n",
    "print(val_pairs.show(5))\n",
    "print(edges.show(5))\n",
    "print(track_features.show(5))\n",
    "print(playlist_features.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e82462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "PARQUET_T_FEATURES = 'parquet_data/track_features'\n",
    "ttab = ds.dataset(PARQUET_T_FEATURES, format='parquet').to_table()\n",
    "tid = np.array(ttab['tid'], dtype=np.int64)\n",
    "aid = np.array(ttab['aid'], dtype=np.int64)\n",
    "alid = np.array(ttab['alid'], dtype=np.int64)\n",
    "tnum = np.stack([\n",
    "    np.array(ttab['z_log_track_cnt'], dtype=np.float32),\n",
    "    np.array(ttab['z_log_artist_cnt'], dtype=np.float32)\n",
    "], axis=1)\n",
    "n_tracks = int(tid.max()) + 1\n",
    "n_artists = int(aid.max()) + 1\n",
    "n_albums = int(alid.max()) + 1\n",
    "\n",
    "aid_by_tid = np.zeros((n_tracks,), dtype=np.int64)\n",
    "alid_by_tid = np.zeros((n_tracks,), dtype=np.int64)\n",
    "tnum_by_tid = np.zeros((n_tracks, 2), dtype=np.float32)\n",
    "\n",
    "aid_by_tid[tid] = aid\n",
    "alid_by_tid[tid] = alid\n",
    "tnum_by_tid[tid] = tnum\n",
    "\n",
    "aid_by_tid = torch.from_numpy(aid_by_tid)\n",
    "alid_by_tid = torch.from_numpy(alid_by_tid)\n",
    "tnum_by_tid = torch.from_numpy(tnum_by_tid)\n",
    "\n",
    "PARQUET_P_FEATURES = 'parquet_data/playlist_features'\n",
    "ptab = ds.dataset(PARQUET_P_FEATURES, format='parquet').to_table()\n",
    "pid = np.array(ptab['pid'])\n",
    "pf = np.stack([\n",
    "    np.array(ptab['log_n_tracks'], dtype=np.float32),\n",
    "    np.array(ptab['log_n_artists'], dtype=np.float32),\n",
    "    np.array(ptab['log_n_albums'], dtype=np.float32),\n",
    "    np.array(ptab['log_pl_duration'], dtype=np.float32),\n",
    "    np.array(ptab['log_days_mod'], dtype=np.float32),\n",
    "    np.array(ptab['collaborative'], dtype=np.float32),\n",
    "], axis=1)\n",
    "\n",
    "n_pids = int(pid.max()) + 1\n",
    "pl_feat_by_pid = np.zeros((n_pids, pf.shape[1]), dtype=np.float32)\n",
    "pl_feat_by_pid[pid] = pf\n",
    "pl_feat_by_pid = torch.from_numpy(pl_feat_by_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a40bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import torch\n",
    "\n",
    "PARQUET_TRAIN = 'parquet_data/train_pairs'\n",
    "PARQUET_VAL = 'parquet_data/val_pairs'\n",
    "\n",
    "n_tracks = 2262108 \n",
    "max_length = 20\n",
    "embed_dim = 128\n",
    "batch_size = 512 # raise if have GPU RAM\n",
    "# Looks for the best GPUs to train on\n",
    "DEVICE = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "class PlaylistPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parquet_path):\n",
    "        table = ds.dataset(parquet_path, format='parquet').to_table(columns=['pid', 'tokens', 'mask', 'pos_tid'])\n",
    "        self.pid = np.asarray(table['pid']).astype(np.int64)\n",
    "        self.tokens = np.stack(table['tokens'].to_pylist()).astype(np.int64)\n",
    "        self.mask = np.stack(table['mask'].to_pylist()).astype(np.float32)\n",
    "        self.pos = np.asarray(table['pos_tid']).astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.pos.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'pid': torch.tensor(self.pid[i], dtype=torch.long),\n",
    "            'tokens': torch.from_numpy(self.tokens[i]),\n",
    "            'mask': torch.from_numpy(self.mask[i]),\n",
    "            'pos': torch.tensor(self.pos[i], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def make_loader(path, batch_size=batch_size, shuffle=True, num_workers=0):\n",
    "    ds_ = PlaylistPairDataset(path)\n",
    "    return ds_, torch.utils.data.DataLoader(\n",
    "        ds_, \n",
    "        batch_size = batch_size, \n",
    "        shuffle=shuffle, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=(DEVICE=='cuda'), \n",
    "        drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63ddf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_tracks,\n",
    "                 n_artists,\n",
    "                 n_albums,\n",
    "                 aid_by_tid,\n",
    "                 alid_by_tid,\n",
    "                 tnum_by_tid,\n",
    "                 pl_feat_by_pid, \n",
    "                 embed_dim=128, \n",
    "                 mlp_hidden=256, \n",
    "                 share_embed=True,\n",
    "                 cat_dim=64):\n",
    "        super().__init__()\n",
    "        # register lookups as buffers so .to(DEVICE) moves them\n",
    "        self.register_buffer('aid_by_tid', aid_by_tid.long())\n",
    "        self.register_buffer('alid_by_tid', alid_by_tid.long())\n",
    "        self.register_buffer('tnum_by_tid', tnum_by_tid.float())\n",
    "        self.register_buffer('pl_feat_by_pid', pl_feat_by_pid.float())\n",
    "        \n",
    "        # id embeddings\n",
    "        self.item_emb = nn.Embedding(n_tracks, embed_dim, padding_idx=0)\n",
    "        self.user_emb = self.item_emb if share_embed else nn.Embedding(n_tracks, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # categorical embeddings\n",
    "        self.artist_emb = nn.Embedding(n_artists, cat_dim, padding_idx=0)\n",
    "        self.album_emb = nn.Embedding(n_albums, cat_dim, padding_idx=0)\n",
    "        \n",
    "        # numeric feature MLPs\n",
    "        self.item_num_mlp = nn.Sequential(\n",
    "            nn.Linear(2, mlp_hidden//2), nn.ReLU(), nn.Linear(mlp_hidden//2, embed_dim)\n",
    "        )\n",
    "        self.user_num_mlp = nn.Sequential(\n",
    "            nn.Linear(pl_feat_by_pid.size(1), mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, embed_dim)\n",
    "        )\n",
    "        # fuse + project\n",
    "        self.item_fuse = nn.Linear(embed_dim + cat_dim + cat_dim + embed_dim, embed_dim)\n",
    "        self.user_fuse = nn.Linear(embed_dim + embed_dim, embed_dim)\n",
    "\n",
    "        self.tau = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    # towers    \n",
    "    def playlist_forward(self, pids, tokens, mask):\n",
    "        # pooled sequence embedding\n",
    "        e = self.user_emb(tokens)\n",
    "        e = e * mask.unsqueeze(-1)\n",
    "        denom = mask.sum(dim=1, keepdims=True).clamp_min(1.0)\n",
    "        pooled = e.sum(dim=1) / denom\n",
    "        seq_vec = F.normalize(pooled, dim=-1)\n",
    "        \n",
    "        # playlist numeric features\n",
    "        pf = self.pl_feat_by_pid[pids]\n",
    "        pf_vec = F.normalize(self.user_num_mlp(pf), dim=-1)\n",
    "        \n",
    "        fused = torch.cat([seq_vec, pf_vec], dim=-1)\n",
    "        return F.normalize(self.user_fuse(fused), dim=-1)\n",
    "    \n",
    "    def track_forward(self, pos_ids):\n",
    "        base = self.item_emb(pos_ids)\n",
    "        a = self.artist_emb(self.aid_by_tid[pos_ids])\n",
    "        al = self.album_emb(self.alid_by_tid[pos_ids])\n",
    "        xn = self.item_num_mlp(self.tnum_by_tid[pos_ids])\n",
    "        fused = torch.cat([base, a, al, xn], dim=-1)\n",
    "        return F.normalize(self.item_fuse(fused), dim=-1)\n",
    "    \n",
    "    def forward(self, pids, tokens, mask, pos):\n",
    "        p = self.playlist_forward(pids, tokens, mask)\n",
    "        t = self.track_forward(pos)\n",
    "        logits = (p @ t.T) / self.tau.clamp_min(.001)\n",
    "        return logits\n",
    "\n",
    "model = TwoTower(\n",
    "    n_tracks=n_tracks,\n",
    "    n_artists=n_artists,\n",
    "    n_albums=n_albums,\n",
    "    aid_by_tid=aid_by_tid,\n",
    "    alid_by_tid=alid_by_tid,\n",
    "    tnum_by_tid=tnum_by_tid,\n",
    "    pl_feat_by_pid=pl_feat_by_pid,\n",
    "    embed_dim=embed_dim,\n",
    "    mlp_hidden=256,\n",
    "    share_embed=True,\n",
    "    cat_dim=64\n",
    ").to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=.0003, weight_decay=.0001)\n",
    "scaler = torch.amp.GradScaler() if DEVICE=='cuda' else None\n",
    "    \n",
    "def train_one_epoch():\n",
    "    model.train(); running=0.0\n",
    "    for batch in train_loader:\n",
    "        pids = batch['pid'].to(DEVICE)\n",
    "        tokens = batch['tokens'].to(DEVICE)\n",
    "        mask = batch['mask'].to(DEVICE)\n",
    "        pos = batch['pos'].to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.amp.autocast():\n",
    "                logits =  model(pids, tokens, mask, pos)\n",
    "                targets = torch.arange(logits.size(0), device=logits.device)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt) \n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(pids, tokens, mask, pos)\n",
    "            targets = torch.arange(logits.size(0), device=logits.device)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "        running += loss.item()\n",
    "    return running / max(1, len(train_loader))\n",
    "\n",
    "def recall_at_k(loader, K=10, pool=2000):\n",
    "    model.eval()\n",
    "    cand = []\n",
    "    for i, b in enumerate(loader):\n",
    "        cand.extend(b['pos'].tolist())\n",
    "        if len(cand) >= pool:\n",
    "            break\n",
    "    cand = torch.tensor(cand[:pool], device=DEVICE)\n",
    "    \n",
    "    hits = n = 0\n",
    "    for b in loader:\n",
    "        pids = b['pid'].to(DEVICE)\n",
    "        tokens = b['tokens'].to(DEVICE) \n",
    "        mask = b['mask'].to(DEVICE)\n",
    "        pos = b['pos'].to(DEVICE)\n",
    "        \n",
    "        p = model.playlist_forward(tokens, mask)\n",
    "        batch = pos.size(0)\n",
    "        pool_ids = cand.unsqueeze(0).repeat(batch, 1)\n",
    "        pos = pool_ids[:, 0]\n",
    "        t = model.track_forward(pool_ids.reshape(-1))\n",
    "        t = t.view(batch, -1, p.size(1))\n",
    "        scores = (p.unsqueeze(1) * t).sum(-1)\n",
    "        topk = scores.topk(K, dim=1).indices\n",
    "        hits += (topk[:, 0] == 0).sum().item()\n",
    "        n += batch\n",
    "    return hits / max(1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "113f4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def bench_loader(loader, steps=200):\n",
    "    it = iter(loader)\n",
    "    # warmup a few batches (jit, caches)\n",
    "    for _ in range(10):\n",
    "        b = next(it)\n",
    "    t0 = time.time()\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            try:\n",
    "                b = next(it)\n",
    "            except StopIteration:\n",
    "                it = iter(loader); b = next(it)\n",
    "            # minimal forward to include model compute cost\n",
    "            logits = model(\n",
    "                b['pid'].to(DEVICE),\n",
    "                b['tokens'].to(DEVICE),\n",
    "                b['mask'].to(DEVICE),\n",
    "                b['pos'].to(DEVICE)\n",
    "            )\n",
    "            n += 1\n",
    "    dt = time.time() - t0\n",
    "    print(f\"{n/dt:.2f} batches/sec  | {len(b['pos'])*(n/dt):.0f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d5cc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.70 batches/sec  | 53607 samples/sec\n"
     ]
    }
   ],
   "source": [
    "train_ds, train_loader = make_loader(PARQUET_TRAIN, shuffle=True, num_workers=0)\n",
    "val_ds,   val_loader   = make_loader(PARQUET_VAL,   shuffle=False, num_workers=0)\n",
    "\n",
    "bench_loader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d127c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        b['pid'][:8].to(DEVICE),\n",
    "        b['tokens'][:8].to(DEVICE),\n",
    "        b['mask'][:8].to(DEVICE),\n",
    "        b['pos'][:8].to(DEVICE)\n",
    "    )\n",
    "print('logits', logits.shape)  # expect [8, 8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
